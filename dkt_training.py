############################################################
# dkt_training.py
# ---------------
# This script trains a Deep Knowledge Tracing (DKT) model
# using the skill/correctness arrays generated by
# data_preparation.py.
#
# Usage example:
#   python dkt_training.py \
#       --data_dir="./prepared_data" \
#       --batch_size=64 \
#       --hidden_size=64 \
#       --num_epochs=5 \
#       --learning_rate=1e-3
#
############################################################

import argparse
import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

##########################
# 1. PARSE ARGUMENTS
##########################

def parse_args():
    parser = argparse.ArgumentParser(description="Train a DKT model.")
    parser.add_argument(
        "--data_dir", type=str, default=".",
        help="Directory where skill_sequences.npy, correctness_sequences.npy "
             "and skill2idx_map.npy are stored."
    )
    parser.add_argument(
        "--batch_size", type=int, default=64,
        help="Batch size for DataLoader."
    )
    parser.add_argument(
        "--hidden_size", type=int, default=64,
        help="Hidden size for the LSTM in the DKT model."
    )
    parser.add_argument(
        "--num_epochs", type=int, default=5,
        help="Number of epochs to train."
    )
    parser.add_argument(
        "--learning_rate", type=float, default=1e-3,
        help="Learning rate for the Adam optimizer."
    )
    parser.add_argument(
        "--device", type=str, default="cpu",
        help="'cpu' or 'cuda'."
    )
    return parser.parse_args()

##########################
# 2. DATASET DEFINITION
##########################

class RealKTData(Dataset):
    """
    A simple Dataset that loads pre-saved skill and correctness
    sequences (each row is a student's entire sequence).
    """
    def __init__(self, skill_seqs, corr_seqs):
        # skill_seqs: shape (num_students, seq_len)
        # corr_seqs: shape (num_students, seq_len)
        self.skill_seqs = skill_seqs
        self.corr_seqs = corr_seqs

    def __len__(self):
        return len(self.skill_seqs)

    def __getitem__(self, idx):
        # Return a single student's skill and correctness array
        return self.skill_seqs[idx], self.corr_seqs[idx]

def collate_fn(batch, num_skills):
    """
    Collate function to build one-hot encodings and targets
    for the DKT model. Summarizing the approach:
      - input_vec[b, t, s + c*num_skills] = 1
        if skill_seqs[b, t] = s and corr_seqs[b, t] = c
      - target[b, t, next_skill] = next_correctness
    """
    skill_seqs, corr_seqs = zip(*batch)

    skill_seqs = np.stack(skill_seqs, axis=0)  # (batch_size, seq_len)
    corr_seqs = np.stack(corr_seqs, axis=0)    # (batch_size, seq_len)

    skill_seqs = torch.LongTensor(skill_seqs)
    corr_seqs = torch.LongTensor(corr_seqs)

    batch_size, seq_len = skill_seqs.shape

    # Build input vectors
    input_vec = torch.zeros(batch_size, seq_len, 2 * num_skills)
    target = torch.zeros(batch_size, seq_len, num_skills)

    for b in range(batch_size):
        for t in range(seq_len - 1):
            skill_id = skill_seqs[b, t].item()
            c = corr_seqs[b, t].item()
            if skill_id >= 0:  # If we used -1 to pad
                input_vec[b, t, skill_id + c * num_skills] = 1.0

            next_skill_id = skill_seqs[b, t+1].item()
            if next_skill_id >= 0:
                next_correct = corr_seqs[b, t+1].item()
                target[b, t, next_skill_id] = float(next_correct)

    return input_vec, target, skill_seqs, corr_seqs


##########################
# 3. MODEL DEFINITION
##########################

class DKTModel(nn.Module):
    """
    LSTM-based Deep Knowledge Tracing model.
    """
    def __init__(self, num_skills, hidden_size=64):
        super(DKTModel, self).__init__()
        self.num_skills = num_skills
        self.hidden_size = hidden_size
        # LSTM input dimension = 2*num_skills (skill + correctness)
        self.lstm = nn.LSTM(
            input_size=2 * num_skills,
            hidden_size=hidden_size,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_size, num_skills)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        """
        x: shape (batch_size, seq_len, 2 * num_skills)
        returns preds: shape (batch_size, seq_len, num_skills)
        """
        lstm_out, _ = self.lstm(x)
        logits = self.fc(lstm_out)
        preds = self.sigmoid(logits)
        return preds

def train_dkt_model(model, dataloader, num_epochs=5, lr=1e-3, device='cpu'):
    """
    Core training loop for the DKT model.
    """
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = nn.BCELoss()

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0

        for batch in dataloader:
            input_vec, target, skill_seq, corr_seq = batch
            input_vec = input_vec.to(device)
            target = target.to(device)

            optimizer.zero_grad()
            preds = model(input_vec)  # (batch_size, seq_len, num_skills)
            loss = criterion(preds.view(-1), target.view(-1))
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}")


##########################
# 4. MAIN
##########################

def main():
    args = parse_args()

    # 4.1 - LOAD THE PREPARED NUMPY FILES
    skill_path = os.path.join(args.data_dir, "skill_sequences.npy")
    corr_path = os.path.join(args.data_dir, "correctness_sequences.npy")
    skill_map_path = os.path.join(args.data_dir, "skill2idx_map.npy")

    if not (os.path.exists(skill_path) and os.path.exists(corr_path) and os.path.exists(skill_map_path)):
        raise FileNotFoundError(
            "Could not find skill_sequences.npy, correctness_sequences.npy, "
            "or skill2idx_map.npy in the specified data_dir."
        )

    skill_seqs = np.load(skill_path)
    corr_seqs = np.load(corr_path)
    skill2idx_map = np.load(skill_map_path, allow_pickle=True).item()

    # 4.2 - AUTOMATICALLY DETERMINE THE NUMBER OF SKILLS
    # The dict skill2idx_map maps raw skill IDs to an integer index.
    # We find the maximum skill index and add 1.
    unique_skill_indices = set(skill2idx_map.values())
    num_skills = len(unique_skill_indices)
    # If skill2idx_map used "overflow" logic, the max index is still within this set.

    print(f"Detected num_skills={num_skills} from skill2idx_map.")
    print(f"skill_sequences shape={skill_seqs.shape}, correctness_sequences shape={corr_seqs.shape}")

    # 4.3 - CREATE DATASET & DATALOADER
    dataset = RealKTData(skill_seqs, corr_seqs)
    dataloader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=lambda x: collate_fn(x, num_skills)
    )

    # 4.4 - INSTANTIATE AND TRAIN THE MODEL
    dkt_model = DKTModel(num_skills=num_skills, hidden_size=args.hidden_size)
    train_dkt_model(
        model=dkt_model,
        dataloader=dataloader,
        num_epochs=args.num_epochs,
        lr=args.learning_rate,
        device=args.device
    )

    # 4.5 - DEMO INFERENCE (optional)
    # Let's do a quick inference on a single mini-batch
    dkt_model.eval()
    with torch.no_grad():
        sample_batch = next(iter(dataloader))
        input_vec, target, skill_seq, corr_seq = sample_batch
        input_vec = input_vec.to(args.device)
        preds = dkt_model(input_vec)
        print("\n--- SAMPLE INFERENCE ---")
        print("Predictions shape:", preds.shape)
        # preds[b, t, s] => prob that user b will be correct on skill s at next step t+1

    print("\nFinished training. Model is ready.")

if __name__ == "__main__":
    main()
    # python dkt_training.py --data_dir="./prepared_data" --batch_size=64 --hidden_size=64 --num_epochs=5 --learning_rate=1e-3 --device="cpu"
